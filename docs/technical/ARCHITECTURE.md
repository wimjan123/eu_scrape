# System Architecture

## ðŸ—ï¸ Overview

The EU Parliament Debates Scraper is designed as a modular, scalable data collection and processing system. The architecture follows a pipeline pattern with clear separation of concerns between data discovery, collection, processing, and output generation.

---

## ðŸŽ¯ Architecture Principles

- **Modular Design**: Independent components with clear interfaces
- **Fault Tolerance**: Graceful handling of API failures and network issues  
- **Scalability**: Support for parallel processing and incremental updates
- **Data Quality**: Built-in validation and quality assurance at each stage
- **Observability**: Comprehensive logging and monitoring capabilities

---

## ðŸ“Š System Components

```mermaid
graph TB
    A[Data Discovery] --> B[Session Collection]
    B --> C[Content Extraction] 
    C --> D[Speaker Resolution]
    D --> E[Content Classification]
    E --> F[Data Validation]
    F --> G[Output Generation]
    
    H[Configuration] --> A
    H --> B
    H --> C
    
    I[Monitoring] --> A
    I --> B
    I --> C
    I --> D
    I --> E
    I --> F
    I --> G
    
    J[Quality Assurance] --> F
    J --> G
```

### 1. Data Discovery Service
**Purpose**: Identify and catalog available plenary sessions  
**Inputs**: Date ranges, session types, language preferences  
**Outputs**: Session metadata with URIs and identifiers  
**Technologies**: SPARQL queries, EU Publications Office API

### 2. Session Collection Service  
**Purpose**: Retrieve structured session data and transcripts  
**Inputs**: Session identifiers from Data Discovery  
**Outputs**: Raw session data with speaker assignments  
**Technologies**: REST APIs, HTML parsing, XML processing

### 3. Content Extraction Service
**Purpose**: Parse and structure speech segments from raw data  
**Inputs**: Raw session transcripts and metadata  
**Outputs**: Individual speech segments with timestamps  
**Technologies**: Text processing, regex patterns, NLP libraries

### 4. Speaker Resolution Service
**Purpose**: Enhance speaker data with country and party information  
**Inputs**: Speaker names and basic metadata  
**Outputs**: Enriched speaker data with affiliations  
**Technologies**: MEP database queries, fuzzy matching, caching

### 5. Content Classification Service
**Purpose**: Identify and label announcements vs. speeches  
**Inputs**: Speech segments with content and speaker metadata  
**Outputs**: Classifications and announcement type labels  
**Technologies**: Rule-based classification, pattern matching

### 6. Data Validation Service
**Purpose**: Ensure data quality and completeness  
**Inputs**: Processed speech segments  
**Outputs**: Validated data with quality metrics  
**Technologies**: Data validation frameworks, statistical analysis

### 7. Output Generation Service
**Purpose**: Format and export final datasets  
**Inputs**: Validated speech segments  
**Outputs**: CSV, JSONL, and documentation files  
**Technologies**: Pandas, JSON libraries, template engines

---

## ðŸ”„ Data Flow Architecture

### Phase 1: Discovery & Collection
```
Configuration â†’ Data Discovery â†’ Session List â†’ Session Collection â†’ Raw Data
```

### Phase 2: Processing & Enhancement  
```
Raw Data â†’ Content Extraction â†’ Speaker Resolution â†’ Content Classification â†’ Processed Data
```

### Phase 3: Validation & Output
```
Processed Data â†’ Data Validation â†’ Quality Metrics â†’ Output Generation â†’ Final Datasets
```

---

## ðŸ“ Directory Structure

```
eu_scrape/
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ discovery/                 # Data discovery services
â”‚   â”‚   â”œâ”€â”€ sparql_client.py      # SPARQL query execution
â”‚   â”‚   â”œâ”€â”€ session_finder.py     # Session discovery logic
â”‚   â”‚   â””â”€â”€ metadata_collector.py # Metadata aggregation
â”‚   â”œâ”€â”€ collection/               # Data collection services
â”‚   â”‚   â”œâ”€â”€ api_client.py         # REST API client
â”‚   â”‚   â”œâ”€â”€ html_parser.py        # HTML content parsing
â”‚   â”‚   â””â”€â”€ xml_processor.py      # XML data processing
â”‚   â”œâ”€â”€ processing/               # Data processing services
â”‚   â”‚   â”œâ”€â”€ content_extractor.py  # Speech segment extraction
â”‚   â”‚   â”œâ”€â”€ speaker_resolver.py   # Speaker data enhancement
â”‚   â”‚   â””â”€â”€ classifier.py         # Content classification
â”‚   â”œâ”€â”€ validation/               # Data validation services
â”‚   â”‚   â”œâ”€â”€ quality_checker.py    # Quality assurance
â”‚   â”‚   â”œâ”€â”€ completeness.py       # Data completeness checks
â”‚   â”‚   â””â”€â”€ accuracy.py           # Accuracy validation
â”‚   â”œâ”€â”€ output/                   # Output generation
â”‚   â”‚   â”œâ”€â”€ csv_exporter.py       # CSV file generation
â”‚   â”‚   â”œâ”€â”€ jsonl_exporter.py     # JSONL file generation
â”‚   â”‚   â””â”€â”€ documentation.py      # README generation
â”‚   â”œâ”€â”€ core/                     # Core utilities
â”‚   â”‚   â”œâ”€â”€ config.py             # Configuration management
â”‚   â”‚   â”œâ”€â”€ logging.py            # Logging setup
â”‚   â”‚   â”œâ”€â”€ monitoring.py         # Health checks and metrics
â”‚   â”‚   â””â”€â”€ exceptions.py         # Custom exceptions
â”‚   â””â”€â”€ main.py                   # Main orchestration script
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ unit/                     # Unit tests
â”‚   â”œâ”€â”€ integration/              # Integration tests
â”‚   â””â”€â”€ data/                     # Test data fixtures
â”œâ”€â”€ config/                       # Configuration files
â”‚   â”œâ”€â”€ production.yaml           # Production settings
â”‚   â”œâ”€â”€ development.yaml          # Development settings
â”‚   â””â”€â”€ logging.yaml              # Logging configuration
â”œâ”€â”€ data/                         # Data storage
â”‚   â”œâ”€â”€ raw/                      # Raw collected data
â”‚   â”œâ”€â”€ processed/                # Processed data
â”‚   â”œâ”€â”€ output/                   # Final output files
â”‚   â””â”€â”€ cache/                    # Cached API responses
â””â”€â”€ scripts/                      # Utility scripts
    â”œâ”€â”€ setup.py                  # Environment setup
    â”œâ”€â”€ run_pipeline.py           # Pipeline execution
    â””â”€â”€ validate_output.py        # Output validation
```

---

## ðŸ”Œ Component Interfaces

### Configuration Interface
```python
@dataclass
class ProcessingConfig:
    date_range: Tuple[date, date]
    languages: List[str]
    output_formats: List[str]
    quality_thresholds: Dict[str, float]
    api_rate_limits: Dict[str, int]
    parallel_workers: int = 4
```

### Data Models
```python
@dataclass  
class SessionMetadata:
    session_id: str
    date: date
    title: str
    session_type: str
    language: str
    source_uri: str

@dataclass
class SpeechSegment:
    speaker_name: str
    speaker_country: str
    speaker_party_or_group: str
    segment_start_ts: datetime
    segment_end_ts: datetime  
    speech_text: str
    is_announcement: bool
    announcement_label: Optional[str]
    quality_score: float
```

### Service Interfaces
```python
class DataDiscoveryService:
    def find_sessions(self, config: ProcessingConfig) -> List[SessionMetadata]:
        """Discover available sessions within date range."""
        pass
    
class ContentExtractionService:
    def extract_segments(self, raw_data: RawSession) -> List[SpeechSegment]:
        """Extract speech segments from raw session data."""
        pass

class ValidationService:
    def validate_segments(self, segments: List[SpeechSegment]) -> ValidationReport:
        """Validate segment data quality and completeness."""
        pass
```

---

## âš¡ Performance Architecture

### Parallel Processing Strategy
- **Session-Level Parallelism**: Process multiple sessions concurrently
- **Service-Level Parallelism**: Independent services can run in parallel
- **I/O Optimization**: Async HTTP requests with connection pooling
- **Caching Strategy**: API response caching to minimize redundant requests

### Scalability Considerations
- **Memory Management**: Stream processing for large datasets
- **Disk Usage**: Efficient temporary file management
- **Network Efficiency**: Request batching and compression
- **Resource Monitoring**: CPU and memory usage tracking

---

## ðŸ›¡ï¸ Error Handling Architecture

### Error Categories
1. **Network Errors**: API timeouts, connection failures
2. **Data Errors**: Malformed responses, missing fields
3. **Processing Errors**: Classification failures, validation errors
4. **System Errors**: Disk space, memory limitations

### Recovery Strategies
```python
class ErrorHandler:
    def handle_network_error(self, error: NetworkError, context: ProcessingContext):
        """Implement exponential backoff retry logic."""
        pass
    
    def handle_data_error(self, error: DataError, context: ProcessingContext):
        """Log error and continue with partial data."""
        pass
    
    def handle_system_error(self, error: SystemError, context: ProcessingContext):
        """Graceful shutdown and state preservation."""
        pass
```

---

## ðŸ“Š Monitoring Architecture

### Health Checks
- **API Connectivity**: Monitor EU Parliament API availability
- **Data Quality**: Track quality metrics over time
- **Processing Performance**: Monitor throughput and latency
- **System Resources**: CPU, memory, and disk usage

### Metrics Collection
```python
class MetricsCollector:
    def collect_processing_metrics(self) -> Dict[str, float]:
        return {
            'sessions_processed_per_hour': 0.0,
            'segments_extracted_per_session': 0.0,
            'speaker_resolution_rate': 0.0,
            'classification_accuracy': 0.0,
            'data_quality_score': 0.0
        }
```

---

## ðŸ”— Cross-References

### Related Documentation
- [API Integration](./API_INTEGRATION.md) - Detailed API specifications
- [Data Schema](../data/DATA_SCHEMA.md) - Data model definitions
- [Configuration](../operations/CONFIGURATION.md) - System configuration
- [Quality Assurance](../data/QUALITY_ASSURANCE.md) - Quality validation procedures

### External Dependencies
- **Python Libraries**: requests, pandas, lxml, pydantic
- **EU APIs**: Publications Office SPARQL, EUR-Lex REST API
- **Data Formats**: JSON, CSV, JSONL, XML, HTML

---

*Last updated: 2025-08-28 | Next review: Implementation phase*